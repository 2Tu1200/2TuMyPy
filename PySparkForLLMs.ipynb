{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tu212223/notebook89abe5249c?scriptVersionId=230656739\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"778ac82e-a167-4208-bb4b-06a506b60ae0","cell_type":"markdown","source":"# Building a PySpark-Powered LLM Data Processing System\nThis guide will help me (2tu/SKS) to create a project that combines PySpark for distributed data processing with local LLM (Llama) and OpenAI API capabilities, and subsequently compare them. Here's a comprehensive plan:\n\nProject Architecture Overview\nData Ingestion Layer: PySpark for loading and preprocessing large datasets\n\nProcessing Layer: Distributed NLP tasks using PySpark\n\nLLM Integration: Local Llama model and OpenAI API for advanced text processing\n\nOutput/Storage: Processed results stored efficiently\n\nStep 1: Setting Up environment prerequisites:\n\nJava 8/11: installed\n\nPython 3.7+: installed\n\nApache Spark: installed\n\nJupyter Notebook: Installed and used since high school.\n\nLlama model: downloaded","metadata":{}},{"id":"584b432b-4fc7-41f4-a3fa-5770343db370","cell_type":"markdown","source":"","metadata":{}},{"id":"93524478-0f89-4812-8909-632f47757e04","cell_type":"markdown","source":"## Installation","metadata":{}},{"id":"d07eb972-1b1e-4d1f-b985-c95e0275cad7","cell_type":"markdown","source":"# Create virtual environment\npython -m venv llm-spark-env\nsource llm-spark-env/bin/activate  # Linux/Mac\n# llm-spark-env\\Scripts\\activate  # Windows\n\n# Install required packages\npip install pyspark jupyter openai transformers torch","metadata":{}},{"id":"d3d54353-6a1a-4696-99b5-e6da7936acc5","cell_type":"markdown","source":"# Step 2: PySpark Initialization\nCreate a utils.py file for Spark session management:","metadata":{}},{"id":"ad9907c7-a2fc-4a04-b898-2d412129c24e","cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark import SparkConf\n\ndef create_spark_session(app_name=\"LLM-Spark-Processing\"):\n    conf = SparkConf() \\\n        .setAppName(app_name) \\\n        .set(\"spark.executor.memory\", \"4g\") \\\n        .set(\"spark.driver.memory\", \"4g\") \\\n        .set(\"spark.sql.shuffle.partitions\", \"100\")\n    \n    spark = SparkSession.builder \\\n        .config(conf=conf) \\\n        .getOrCreate()\n    \n    return spark\n\ndef stop_spark_session(spark):\n    spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:13:41.048546Z","iopub.execute_input":"2025-03-31T06:13:41.048863Z","iopub.status.idle":"2025-03-31T06:13:41.372648Z","shell.execute_reply.started":"2025-03-31T06:13:41.048828Z","shell.execute_reply":"2025-03-31T06:13:41.37093Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_spark_session\u001b[39m(app_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM-Spark-Processing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error"}],"execution_count":1},{"id":"7ba49fc6-d35c-4687-ad03-b01afd59df06","cell_type":"markdown","source":"# While look-wise, this kaggle notebook seems a real attractive, intutive and best notebook, cannot say why there is no module error: ModuleNotFoundError, if the spark session has been successfuly created. ","metadata":{}},{"id":"4508c2cf-58fb-472e-b841-e294168c180d","cell_type":"markdown","source":"# Step 3: Data Processing with PySpark\nCreate a data_processor.py:","metadata":{}},{"id":"0b094045-3d87-4b06-951e-e16e10267639","cell_type":"code","source":"from pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType, ArrayType\nfrom utils import create_spark_session\n\nclass DataProcessor:\n    def __init__(self):\n        self.spark = create_spark_session()\n        \n    def load_data(self, file_path, file_type=\"csv\", **options):\n        \"\"\"Load data from various sources\"\"\"\n        if file_type == \"csv\":\n            return self.spark.read.csv(file_path, **options)\n        elif file_type == \"json\":\n            return self.spark.read.json(file_path, **options)\n        elif file_type == \"parquet\":\n            return self.spark.read.parquet(file_path, **options)\n        else:\n            raise ValueError(f\"Unsupported file type: {file_type}\")\n            \n    def basic_clean(self, df, text_column):\n        \"\"\"Basic text cleaning\"\"\"\n        from pyspark.sql.functions import trim, lower, regexp_replace\n        \n        return df.withColumn(text_column, trim(col(text_column))) \\\n                 .withColumn(text_column, lower(col(text_column))) \\\n                 .withColumn(text_column, regexp_replace(col(text_column), r\"[^\\w\\s]\", \"\"))\n    \n    def process_large_texts(self, df, text_column, chunk_size=512):\n        \"\"\"Chunk large texts for LLM processing\"\"\"\n        @udf(ArrayType(StringType()))\n        def chunk_text(text):\n            if not text:\n                return []\n            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n            \n        return df.withColumn(f\"{text_column}_chunks\", chunk_text(col(text_column)))\n    \n    def save_processed_data(self, df, output_path, output_format=\"parquet\"):\n        \"\"\"Save processed data\"\"\"\n        if output_format == \"parquet\":\n            df.write.parquet(output_path, mode=\"overwrite\")\n        elif output_format == \"csv\":\n            df.write.csv(output_path, mode=\"overwrite\", header=True)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"bf8240d4-04aa-4824-989e-b3a639b40b4e","cell_type":"markdown","source":"# Step 4: LLM Integration\nCreate llm_integration.py","metadata":{}},{"id":"9633e3ca-6185-450c-9f29-51294d517f1f","cell_type":"code","source":"from transformers import pipeline\nfrom openai import OpenAI\nimport os\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, ArrayType\n\nclass LLMIntegration:\n    def __init__(self, local_model_path=None, openai_key=None):\n        self.local_llm = None\n        self.openai_client = None\n        \n        if local_model_path:\n            self._load_local_model(local_model_path)\n        if openai_key:\n            self._setup_openai(openai_key)\n    \n    def _load_local_model(self, model_path):\n        \"\"\"Load local Llama model\"\"\"\n        print(\"Loading local Llama model...\")\n        self.local_llm = pipeline(\n            \"text-generation\",\n            model=model_path,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n        print(\"Local model loaded successfully\")\n    \n    def _setup_openai(self, api_key):\n        \"\"\"Initialize OpenAI client\"\"\"\n        self.openai_client = OpenAI(api_key=api_key)\n    \n    def get_local_llm_udf(self, max_length=50):\n        \"\"\"Create PySpark UDF for local LLM\"\"\"\n        def generate_text(prompt):\n            if not self.local_llm or not prompt:\n                return \"\"\n            result = self.local_llm(\n                prompt,\n                max_length=max_length,\n                do_sample=True,\n                temperature=0.7\n            )\n            return result[0]['generated_text']\n        \n        return udf(generate_text, StringType())\n    \n    def get_openai_udf(self, model=\"gpt-3.5-turbo\", max_tokens=50):\n        \"\"\"Create PySpark UDF for OpenAI API\"\"\"\n        def generate_text(prompt):\n            if not self.openai_client or not prompt:\n                return \"\"\n            try:\n                response = self.openai_client.chat.completions.create(\n                    model=model,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    max_tokens=max_tokens\n                )\n                return response.choices[0].message.content\n            except Exception as e:\n                print(f\"OpenAI API error: {e}\")\n                return \"\"\n        \n        return udf(generate_text, StringType())\n    \n    def batch_process_with_local_llm(self, spark_df, text_column, output_column=\"llm_output\"):\n        \"\"\"Batch process text column with local LLM\"\"\"\n        llm_udf = self.get_local_llm_udf()\n        return spark_df.withColumn(output_column, llm_udf(col(text_column)))\n    \n    def batch_process_with_openai(self, spark_df, text_column, output_column=\"openai_output\"):\n        \"\"\"Batch process text column with OpenAI\"\"\"\n        openai_udf = self.get_openai_udf()\n        return spark_df.withColumn(output_column, openai_udf(col(text_column)))","metadata":{},"outputs":[],"execution_count":null},{"id":"8d6ebd98-6f0d-4e20-8586-c0b3d4fb9f42","cell_type":"markdown","source":"# Step 5: Main Application\nCreate main.py:","metadata":{}},{"id":"407422fb-6285-4a77-acec-6300cf423b28","cell_type":"code","source":"from data_processor import DataProcessor\nfrom llm_integration import LLMIntegration\nfrom utils import create_spark_session, stop_spark_session\nimport time\n\ndef main():\n    # Initialize\n    spark = create_spark_session(\"LLM-Spark-Project\")\n    processor = DataProcessor()\n    llm_integration = LLMIntegration(\n        local_model_path=\"path/to/your/llama/model\",  # Update this\n        openai_key=\"your_openai_key\"  # Optional, update if needed\n    )\n    \n    try:\n        # Load and process data\n        print(\"Loading and processing data...\")\n        df = processor.load_data(\"data/input_data.csv\", header=True, inferSchema=True)\n        cleaned_df = processor.basic_clean(df, \"text_column\")\n        chunked_df = processor.process_large_texts(cleaned_df, \"text_column\")\n        \n        # LLM Processing\n        print(\"Processing with local LLM...\")\n        start_time = time.time()\n        local_llm_df = llm_integration.batch_process_with_local_llm(chunked_df, \"text_column_chunks\")\n        print(f\"Local LLM processing took {time.time() - start_time:.2f} seconds\")\n        \n        # Optional OpenAI Processing\n        if llm_integration.openai_client:\n            print(\"Processing with OpenAI...\")\n            start_time = time.time()\n            openai_df = llm_integration.batch_process_with_openai(chunked_df, \"text_column_chunks\")\n            print(f\"OpenAI processing took {time.time() - start_time:.2f} seconds\")\n        \n        # Save results\n        processor.save_processed_data(local_llm_df, \"data/output/local_llm_results\")\n        if llm_integration.openai_client:\n            processor.save_processed_data(openai_df, \"data/output/openai_results\")\n        \n        print(\"Processing complete!\")\n        \n    finally:\n        stop_spark_session(spark)\n\nif __name__ == \"__main__\":\n    main()","metadata":{},"outputs":[],"execution_count":null},{"id":"c38163f6-4d4d-4282-9fbb-3f8b24182149","cell_type":"markdown","source":"# Step 6: Jupyter Notebook Example\nCreate a notebook LLM_Spark_Demo.ipynb:","metadata":{}},{"id":"d680168b-0df9-4b34-a4af-2f411649f744","cell_type":"code","source":"# Initialize\nfrom utils import create_spark_session\nfrom data_processor import DataProcessor\nfrom llm_integration import LLMIntegration\n\nspark = create_spark_session(\"LLM-Spark-Notebook\")\nprocessor = DataProcessor()\nllm = LLMIntegration(local_model_path=\"path/to/your/llama/model\")\n\n# Sample data processing\nsample_data = [(\"This is a sample text to process.\", 1),\n               (\"Another example of text data for analysis.\", 2)]\n               \ndf = spark.createDataFrame(sample_data, [\"text\", \"id\"])\ndisplay(df)\n\n# Process with local LLM\nprocessed_df = llm.batch_process_with_local_llm(df, \"text\")\ndisplay(processed_df)\n\n# Stop Spark when done\nspark.stop()","metadata":{},"outputs":[],"execution_count":null},{"id":"2f65c3fd-f795-479d-93fc-64504120d932","cell_type":"markdown","source":"# Step 7: Performance Optimization\nAdd to data_processor.py:","metadata":{}},{"id":"b6a03cc8-0208-4329-adff-e1b193c70dff","cell_type":"code","source":"def optimize_for_llm_processing(self, df, text_column, cache=True):\n    \"\"\"Optimize DataFrame for LLM processing\"\"\"\n    # Repartition based on text length for better load balancing\n    from pyspark.sql.functions import length\n    \n    df = df.withColumn(\"text_length\", length(col(text_column)))\n    df = df.repartition(\"text_length\")\n    \n    if cache:\n        df.cache()\n    \n    return df\n\ndef parallel_llm_processing(self, df, processing_function, batch_size=100):\n    \"\"\"Process data in parallel batches\"\"\"\n    from pyspark.sql.functions import pandas_udf\n    import pandas as pd\n    \n    @pandas_udf(StringType())\n    def batch_process(texts: pd.Series) -> pd.Series:\n        return texts.apply(processing_function)\n    \n    return df.withColumn(\"processed_text\", batch_process(col(\"text_chunk\")))","metadata":{},"outputs":[],"execution_count":null},{"id":"3e156d5e-18d8-4bac-9e2e-54ea8d6cfb5c","cell_type":"markdown","source":"# Step 8: Error Handling and Logging\nCreate logging_utils.py:","metadata":{}},{"id":"9946797d-0a30-4926-b88d-5487ac4a6479","cell_type":"code","source":"import logging\nfrom pyspark import SparkContext\n\ndef setup_logging():\n    \"\"\"Configure logging for both Python and Spark\"\"\"\n    # Python logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('logs/llm_spark.log'),\n            logging.StreamHandler()\n        ]\n    )\n    \n    # Spark logging\n    sc = SparkContext.getOrCreate()\n    log4jLogger = sc._jvm.org.apache.log4j\n    logger = log4jLogger.LogManager.getLogger(__name__)\n    logger.info(\"Spark logging initialized\")\n    \n    return logger\n\ndef log_errors(func):\n    \"\"\"Decorator for error logging\"\"\"\n    def wrapper(*args, **kwargs):\n        logger = setup_logging()\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logger.error(f\"Error in {func.__name__}: {str(e)}\", exc_info=True)\n            raise\n    return wrapper","metadata":{},"outputs":[],"execution_count":null},{"id":"6871b682-e7f6-4d34-84fb-c69801ac9ca7","cell_type":"markdown","source":"# Project Structure Recommendation","metadata":{}},{"id":"0b481c59-09e5-4b6d-985f-57898da9b1c0","cell_type":"code","source":"llm-spark-project/\n│\n├── data/\n│   ├── input/            # Raw input data\n│   └── output/           # Processed output\n│\n├── logs/                 # Log files\n│\n├── src/\n│   ├── __init__.py\n│   ├── main.py           # Main application\n│   ├── data_processor.py # PySpark data processing\n│   ├── llm_integration.py # LLM integration\n│   ├── utils.py          # Spark utilities\n│   └── logging_utils.py  # Logging configuration\n│\n├── notebooks/            # Jupyter notebooks\n│   └── LLM_Spark_Demo.ipynb\n│\n├── requirements.txt      # Python dependencies\n│\n├── README.md            # Project documentation\n└── .gitignore","metadata":{},"outputs":[],"execution_count":null},{"id":"4f7842fd-0da1-4db8-a3b3-e3fbe71543ac","cell_type":"markdown","source":"# Additional Recommendations for My GitHub Project\n## Documentation: Have to create a detailed README.md explaining:\n\n## Project purpose\n\n## Setup instructions\n\n## Configuration options\n\n## Example use cases\n\n## Sample Datasets: Have to include small sample datasets to demonstrate functionality\n\n## Docker Support: Have to consider adding Dockerfile for easy reproducibility\n\n## Benchmarking: Have to add performance comparison between local LLM and OpenAI\n\n## Advanced Features:\n\n## Have to add support for other LLMs\n\n## Have to implement caching mechanisms\n\n## Have to add visualization capabilities for results\n\n## This architecture provides a robust foundation for my project that showcases distributed data processing with PySpark for LLM data processing. The modular design makes it easy to extend and adapt to different use cases.\n","metadata":{}},{"id":"735fefde-fe45-4d81-a11c-b012a9285535","cell_type":"markdown","source":"### The last modified timestamp **********29th March 2025*********","metadata":{}},{"id":"1dc7794a-f3dd-40bb-85b9-611566abb719","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}