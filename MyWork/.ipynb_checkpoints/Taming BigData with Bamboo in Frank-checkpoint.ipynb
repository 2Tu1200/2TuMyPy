{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b5ff67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dc45758520f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RatingsHistogram\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"file:///C:/SparkCourse/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada7d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cff5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('C:\\spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461873c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5701d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"file:///C:/SparkCourse/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d61de0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 343.375)\n",
      "(19, 213.27272727272728)\n",
      "(20, 165.0)\n",
      "(21, 350.875)\n",
      "(22, 206.42857142857142)\n",
      "(23, 246.3)\n",
      "(24, 233.8)\n",
      "(25, 197.45454545454547)\n",
      "(26, 242.05882352941177)\n",
      "(27, 228.125)\n",
      "(28, 209.1)\n",
      "(29, 215.91666666666666)\n",
      "(30, 235.8181818181818)\n",
      "(31, 267.25)\n",
      "(32, 207.9090909090909)\n",
      "(33, 325.3333333333333)\n",
      "(34, 245.5)\n",
      "(35, 211.625)\n",
      "(36, 246.6)\n",
      "(37, 249.33333333333334)\n",
      "(38, 193.53333333333333)\n",
      "(39, 169.28571428571428)\n",
      "(40, 250.8235294117647)\n",
      "(41, 268.55555555555554)\n",
      "(42, 303.5)\n",
      "(43, 230.57142857142858)\n",
      "(44, 282.1666666666667)\n",
      "(45, 309.53846153846155)\n",
      "(46, 223.69230769230768)\n",
      "(47, 233.22222222222223)\n",
      "(48, 281.4)\n",
      "(49, 184.66666666666666)\n",
      "(50, 254.6)\n",
      "(51, 302.14285714285717)\n",
      "(52, 340.6363636363636)\n",
      "(53, 222.85714285714286)\n",
      "(54, 278.0769230769231)\n",
      "(55, 295.53846153846155)\n",
      "(56, 306.6666666666667)\n",
      "(57, 258.8333333333333)\n",
      "(58, 116.54545454545455)\n",
      "(59, 220.0)\n",
      "(60, 202.71428571428572)\n",
      "(61, 256.22222222222223)\n",
      "(62, 220.76923076923077)\n",
      "(63, 384.0)\n",
      "(64, 281.3333333333333)\n",
      "(65, 298.2)\n",
      "(66, 276.44444444444446)\n",
      "(67, 214.625)\n",
      "(68, 269.6)\n",
      "(69, 235.2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"file:///C:/SparkCourse/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "results = averagesByAge.collect()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc5b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
