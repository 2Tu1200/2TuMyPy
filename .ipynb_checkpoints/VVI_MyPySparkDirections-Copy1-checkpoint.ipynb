{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71cc252",
   "metadata": {},
   "source": [
    "#### Hurrah...........to run different Pyspark apps using different Spark sessions, even restart is not required. Log-out is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9868d36",
   "metadata": {},
   "source": [
    "# My PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77272d",
   "metadata": {},
   "source": [
    "PySpark can be easily installed as per the directions mentioned in CJ of my phone or following the instructions in \"Taming Big data by Apache Spark by Frank Kane\". After seeing the \"SPARK\" logo and confirmed that spark has been successfully loaded, \"Findspark\" is the magic module that should be installed with command like \"Pip3 install Findspark\". Run command prompt and type Python to get triple arrow sign of Python command prompt and type \"Import Findspark\" and \"Findspark.init('C:\\spark'), it should run silently, that's correct. Now another important library is Py4J, that I installed by runing \"Pip3 install Py4J\" install ##Now It should run on Jupyter Notebook, as mentioned down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a9ad8",
   "metadata": {},
   "source": [
    "# Initially it was giving error like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812ebd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e99a90",
   "metadata": {},
   "source": [
    "# Now It should run on Jupyter Notebook, through above mentioned steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f151",
   "metadata": {},
   "source": [
    "## Now pyspark is accessible through findspark module, so below 3 lines are necessary to access PySpark in Jupyter Notebook ysstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4913ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e9ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('C:\\spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96560fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2109ff6",
   "metadata": {},
   "source": [
    "## See how smoothly it's being assessed here (above cell) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d59db",
   "metadata": {},
   "source": [
    "# Starting my new SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff6e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7572b60",
   "metadata": {},
   "source": [
    "##Hold-on for 10 secs as starting a Spark Session just takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697e9baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ffe8c",
   "metadata": {},
   "source": [
    "This was the error while I was running above code here, again n again. Now I am trying starting a manual Spark Session through command prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a89ad",
   "metadata": {},
   "source": [
    "## Bastard, Yeh (Manual Spark session, tabhi start ho raha hai jab main internet connected hoon, Khair, now proceed ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31850d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38df86",
   "metadata": {},
   "source": [
    "After Running the manual Spark session and running internet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0874a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0117fb",
   "metadata": {},
   "source": [
    "##Saalaa, Lagataa Hai system restart karke dekhnaa padegaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fc160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41ec16",
   "metadata": {},
   "source": [
    "# Saalaa, system restart aur net se connect karke, yeh \"spark = SparkSession.builder.appName('Basics').getOrCreate()\" sahi men chal gaya. Matlab SparkSession mera PC kaa desktop session me sirf ek hi baar create ho sakta hai. Thanks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c192e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db21995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a01dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffed415",
   "metadata": {},
   "source": [
    "Here (Line number 8)I have to be careful that People.json file should be located in the same directory where JN is currently running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b915b",
   "metadata": {},
   "source": [
    "#Yeh Salaa (Line number 7) achanak chal pada, jab maine 1. \"Pip3 install Py4J\" install kiya. 2. JN restart karke Line number 5 aur 6 ke commands run kiye, matlab pehla app \"Basic\" create ho gaya...Hurrah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22454f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06603ae2",
   "metadata": {},
   "source": [
    "## Saalaa yahan tak toh thik chal raha hai.....ab isko kya hua ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2faf7",
   "metadata": {},
   "source": [
    "Ab Scala daal kar dekhten hain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0986c98",
   "metadata": {},
   "source": [
    "Usase pehle ki Virtual Box or Ubuntu aur Scala dalen, hum Frank bhaiyaa kaa jo Spark instructions hai, wahi follow karten hain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe8ef4",
   "metadata": {},
   "source": [
    "# Now continued from Line number 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b07e13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697817e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf16ba",
   "metadata": {},
   "source": [
    "##Now Happily I am done with installing and accessing PySpark on JN. Also, I am to continue my study in PIERIAN DATA by Jose##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762b0d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'session' is not defined"
     ]
    }
   ],
   "source": [
    "session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8140389f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stop() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: stop() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c04ad8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:601\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;124;03m\"\"\"Stop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m    602\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_sc'"
     ]
    }
   ],
   "source": [
    "SparkSession.stop('self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e658ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047cb9d5",
   "metadata": {},
   "source": [
    "# Just typing (Line number 14) \"exit()\" terminated the kernel and SparkSession, Beta PySpark padhnaa hai toh, specific spark session me (automatic created by system restart ), internet on karke, upar ke sare command run karke, tab karnaa hogaa. **************Keep It Up************  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ec250f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Basics, master=local[*]) created by getOrCreate at C:\\Users\\Tu2\\AppData\\Local\\Temp\\ipykernel_7152\\787016349.py:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRatingsHistogram\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m lines \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///D:/MyJScode/MyWork/ml-100k/u.data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ratings \u001b[38;5;241m=\u001b[39m lines\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\context.py:115\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mCreate a new SparkContext. At least the master and app name should be set,\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03meither through the named parameters here or through C{conf}.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03mValueError:...\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callsite \u001b[38;5;241m=\u001b[39m first_spark_call() \u001b[38;5;129;01mor\u001b[39;00m CallSite(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    118\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\context.py:270\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    267\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[0;32m    275\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Basics, master=local[*]) created by getOrCreate at C:\\Users\\Tu2\\AppData\\Local\\Temp\\ipykernel_7152\\787016349.py:1 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"file:///D:/MyJScode/MyWork/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c76b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"file:///C:/SparkCourse/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ef4aa",
   "metadata": {},
   "source": [
    "# Jio ------------Mere-----------------Lal. Aakhir me uparwala module run kara hi liya maine----------dinbhar lagane ke baad. Mujhe Ye yaad rakhanaa hai ki yeh Python 3.5 ke baad ka version par Spark 2.1.1 kaafi problematic hai. Isliye maine ek virtual environment banaya (conda create myenv) activate kiya (conda activate myenv), python 3.5 install kiya pip ke through, findspark and Py4j install kiya pip ke through, aur finally JN install kiya, aur run kiya.Isliye jab bhi PySpark practice karna hai toh: \n",
    "\n",
    "## On Conda command prompt type \"Conda activate myenv\" and then \"Jupyter Notebook\" start karna hai. Upar ke 3-4 commands, Spark session start karna zaroori nahi hai,  run karna hai. Apna practice karna hai. Phir JN close karke type karna hai \"conda deactivate myenv\". \n",
    "\n",
    "### Internet really is not required for running PySpark\n",
    "\n",
    "# Thanks  and  Happy Learning PySpark: \"Taming Big Data by Frank\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47687b29",
   "metadata": {},
   "source": [
    "#### Hurrah...........to run different Pyspark apps using different Spark sessions, even restart is not required. Log-out is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c24b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
